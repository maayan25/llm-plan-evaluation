# Author: Ma'ayan Armony <maayan.armony@kcl.ac.uk>
# Script to run the full pipeline for evaluation of a plan generated by an LLM against a ground truth plan provided by
# FD in PlanBench.
#
# The pipeline consists of the following steps:
# 1. Evaluate the plan generated by the LLM against the ground truth plan, based on NLP similarity algorithms (see evaluate_plan.py)
# 2. Find a candidate plan that is more similar to the ground truth plan, by consistent variable swap, circular shift (offset), and length penalty
# 3. Compute the number of steps to recover the plan, and the number of steps to recover the candidate plan
# 4. Attempt to improve the plan using the simulated states and actions from the ground truth plan, and finding the last state in the plan
# 5. Regenerate a subset of the plan if needed, and evaluate the plan again
import gc
import io
import os
import pstats
import psutil

from utils import set_configuration, get_plans_from_json, append_evaluation_to_csv, get_default_values_for_csv
from evaluate_plan_per_action import EvaluatePlanPerAction
from initial_plan_recovery import InitialPlanRecovery
import argparse
import concurrent.futures
import threading
import cProfile
import resource

# Please run the file from its directory or modify the paths
current_dir = os.getcwd()
project_dir = os.path.abspath(os.path.join(current_dir, os.pardir, os.pardir))
parent_dir = os.path.abspath(os.path.join(project_dir, os.pardir))

# for writing results to csv (from root dir of the project)
results_dir = f"{project_dir}/results/plan_recovery/final_evaluation/"
tmp_outputs_dir = f"{project_dir}/results/plan_recovery/tmp_outputs/"
plans_dir = f"{current_dir}/plans"

os.makedirs(results_dir, exist_ok=True)
os.makedirs(tmp_outputs_dir, exist_ok=True)
os.makedirs(plans_dir, exist_ok=True)

dict_lock = threading.Lock()

def limit_memory(max_mb: int = 1500):
    try:
        soft, hard = resource.getrlimit(resource.RLIMIT_AS)
        max_bytes = max_mb * 1024 * 1024
        resource.setrlimit(resource.RLIMIT_DATA, (max_bytes, hard))
    except Exception as e:
        print(f"Failed to set memory limit: {e}")

def profile_process_instance(instance, task, domain, model, output_file, params):
    profiler = cProfile.Profile()
    profiler.enable()

    try:
        process_instance(instance, task, domain, model, output_file, params)
    finally:
        profiler.disable()

        # Save stats to a file or print them
        s = io.StringIO()
        ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        ps.print_stats()

        # print(s.getvalue())
        with open(f"profile_instance_{instance[0]}.txt", "w") as f:
            f.write(s.getvalue())

def process_instance(instance, task, domain, model, output_file, params):
    try:
        limit_memory(3000)  # 3GB

        inst_num = instance[0]
        print(f"Processing instance {inst_num}")

        # For PDDL task GT has cost in the last line
        if "pddl" in task:
            gt_plan = "\n".join(instance[2].rstrip("\n").rsplit("\n", 1)[:-1]) + "\n"
        else:
            gt_plan = instance[2]

        if instance[1] is None or gt_plan is None:
            print(f"Instance {inst_num} has missing plan or GT plan")
            default_values = get_default_values_for_csv()
            return inst_num, {}, {}, {}

        # chooser = ChooseGTPlan(parse_plan(instance[1]), [parse_plan(gt_plan)])
        # chooser.choose_gt_plan()

        # Run the evaluation of the plan generated by the LLM
        unique_id = f"{domain}__{model}_{task}__{inst_num}"
        import re
        curr_plan = re.sub(r'\bo(\d+)\b', r'object_\1', instance[1]) if "randomized" in domain else instance[1]
        evaluator = EvaluatePlanPerAction(curr_plan, gt_plan, instance[3], domain, inst_num, unique_id=unique_id) # removed chooser
        evaluator.run_evaluation()

        print(f"Evaluator works, memory usage (MB) for inst {inst_num}:", psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)

        improver = InitialPlanRecovery(evaluator.curr_plan, evaluator, evaluator.gt_plan, domain, inst_num)

        print(f"Improver works, memory usage (MB) for inst {inst_num}:", psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)

        plan_data = {
            "curr_plan": improver.curr_plan.get_plan(),
            "gt_plan": evaluator.gt_plan,
            "cdt_plan": improver.candidate_plan.get_plan(),
            "subseq_plan": improver.subseq_plan.get_plan(),
            "cdt_subseq_plan": improver.subseq_candidate_plan.get_plan(),
            "plan_trace": improver.curr_plan.plan_trace,
            "nr_plan_trace": improver.curr_plan.nr_plan_trace,
            "trace_score": improver.curr_plan.get_trace_score(),
            "complementary_plan": improver.complementary_plan,
            "correct_part": improver.correct_part,
            "final_score": evaluator.get_final_score(),
            "executable": improver.curr_plan.analysis["executable"]
        }

        result = {
            "curr_score": improver.curr_plan.get_score(),
            "curr_steps": len(improver.curr_plan.steps_to_validity),
            "curr_last_exec": improver.curr_plan.get_ind_last_exec(),
            "improved_plan": improver.improved_plan.get_plan(),

            "cdt_score": improver.candidate_plan.get_score(),
            "cdt_steps": len(improver.candidate_plan.steps_to_validity),
            "cdt_last_exec": improver.candidate_plan.get_ind_last_exec(),

            "subseq_score": improver.subseq_plan.get_score(),
            "subseq_steps": len(improver.subseq_plan.steps_to_validity),
            "subseq_last_exec": improver.subseq_plan.get_ind_last_exec(),

            "cdt_subseq_score": improver.subseq_candidate_plan.get_score(),
            "cdt_subseq_steps": len(improver.subseq_candidate_plan.steps_to_validity),
            "cdt_subseq_last_exec": improver.subseq_candidate_plan.get_ind_last_exec(),
        }

        validity = {
            "curr_validity": improver.curr_plan.is_valid,
            "cdt_validity": improver.candidate_plan.is_valid,
            "subseq_validity": improver.subseq_plan.is_valid,
            "cdt_subseq_validity": improver.subseq_candidate_plan.is_valid,
            "improved_validity": improver.improved_plan.is_valid,
        }

        print(f"Memory usage (MB) for inst {inst_num}:", psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)
        print(f"CPU usage (%) for inst {inst_num}:", psutil.Process(os.getpid()).cpu_percent(interval=1))
        # tmp_output_file = os.path.join(tmp_outputs_dir, f"{unique_id}.csv")

        if plan_data is not None:
            # append_evaluation_to_csv(tmp_output_file, inst_num, plan_data, result, validity, params)
            append_evaluation_to_csv(output_file, inst_num, plan_data, result, validity, params)

        # Clear and return just status
        del evaluator, improver
        gc.collect()

        # Remove the plans in the plans directory that start with the unique_id
        for filename in os.listdir(plans_dir):
            if filename.startswith(f"plan_validation_{unique_id}"):
                file_path = os.path.join(plans_dir, filename)
                if os.path.isfile(file_path):
                    os.remove(file_path)

        return inst_num, True
    except MemoryError:
        print(f"MemoryError on instance {instance[0]}")
        return instance[0], False
    except Exception as e:
        print(f"Exception on instance {instance[0]}: {e}")
        return instance[0], False

def main():
    # Argument parser
    parser = argparse.ArgumentParser(description="Run the full pipeline for evaluation of a plan generated by an LLM")
    parser.add_argument("--project", type=str, default="llm_planning_analysis", help="The project to evaluate the plan for")
    parser.add_argument("--domain", type=str, default="logistics", help="The domain to evaluate the plan for")
    parser.add_argument("--model", type=str, default="qcode", help="The model to evaluate the plan for")
    parser.add_argument("--task", type=str, default="", help="The task to evaluate the plan for", choices=["", "_state_tracking", "_pddl", "_zero_shot"])
    parser.add_argument("--specific_instance", type=int, default=None, help="Specific instance number to process (if any)")
    args = parser.parse_args()
    # configuration
    project = args.project  # llm_planning_analysis / plan-bench
    domain = args.domain # blocksworld_3, logistics
    model = args.model  # qwen, llama, qcode, llama_instruct, gemini_2_flash
    task = args.task  # "" -> "_one_shot", "_state_tracking", "_pddl", "_zero_shot"
    specific_instance = args.specific_instance  # None -> process all instances
    temp = 0.1
    top_p = 1
    params = set_configuration(model=model, task=task, domain=domain, project=project, temp=temp, top_p=top_p)

    filepath = f"{parent_dir}/LLM-Planning-PlanBench/{project}/results/{domain}/temp_{temp}_top_p_{top_p}/{model}/task_1_plan_generation{task}.json"

    if len(task) == 0:
        task = "_one_shot"
    output_file = f"{results_dir}/{domain}_evaluation_{model}{task}.csv"  # plan_generation.csv

    try:
        # Get all instances for this configuration
        plans = get_plans_from_json(filepath)
    except FileNotFoundError:
        print(f"Configuration does not exist / File Not Found: {filepath}")
        return
    max_workers = os.cpu_count() * 2 // 3
    # print("DEBUG CONCURRENCY: ", max_workers)
    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        begin = 0
        end = 100 if domain == "logistics" else len(plans)
        if specific_instance is not None:
            offset = 1 if task == "_zero_shot" else 0
            begin = specific_instance + offset - 2
            end = specific_instance + offset - 1
        for instance in plans[begin:end]:
            print(f"Processing instance {instance[0]}")
            future = executor.submit(process_instance, instance=instance, task=task, domain=domain, model=model,
                                     output_file=output_file, params=params)
            try:
                instance_id, is_processed = future.result(timeout=30000) # TODO change back to 300
                if is_processed:
                    print(f"Instance {instance_id} processed successfully.")
                else:
                    print(f"Instance {instance_id} failed to process.")
                    continue
            except concurrent.futures.TimeoutError:
                print(f"Timeout on instance {instance[0]}. Moving on...")
            except Exception as e:
                print(f"Error occurred in instance {instance[0]}: {str(e)}")

    # Remove the plans directory after processing
    # os.remove(f"{current_dir}/plans")

    # write_evaluation_to_csv(output_file, plans_res, curr_scores, params, curr_validity)

if __name__ == "__main__":
    main()
